<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_umyph4ytn2e9-1{list-style-type:none}ul.lst-kix_umyph4ytn2e9-2{list-style-type:none}ul.lst-kix_umyph4ytn2e9-0{list-style-type:none}.lst-kix_umyph4ytn2e9-8>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-7>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-1>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-0>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-2>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-3>li:before{content:"\0025cf  "}ul.lst-kix_umyph4ytn2e9-7{list-style-type:none}.lst-kix_umyph4ytn2e9-4>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-6>li:before{content:"\0025cf  "}ul.lst-kix_umyph4ytn2e9-8{list-style-type:none}ul.lst-kix_umyph4ytn2e9-5{list-style-type:none}ul.lst-kix_umyph4ytn2e9-6{list-style-type:none}ul.lst-kix_umyph4ytn2e9-3{list-style-type:none}ul.lst-kix_umyph4ytn2e9-4{list-style-type:none}.lst-kix_umyph4ytn2e9-5>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c0{padding-top:-5pt;padding-bottom:12pt;line-height:1.3;orphans:2;widows:2}.c6{page-break-after:avoid;orphans:2;widows:2;text-align:center}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c2{color:#1155cc;text-decoration:underline}.c5{color:inherit;text-decoration:inherit}.c3{page-break-after:avoid;height:20pt}.c9{orphans:2;widows:2}.c10{text-indent:36pt}.c8{text-align:center}.c4{font-weight:700}.c1{font-family:"Georgia"}.c12{height:11pt}.c7{page-break-after:avoid}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><p class="c0 c7 c8 title" id="h.ib1fd7q34vnu"><span>PixelDream</span></p><p class="c6 subtitle" id="h.sohaj7fkc0cm"><span>James Reed and Kevin Malhotra</span></p><p class="c6 subtitle" id="h.ykdup3ugyj7c"><span>ECE 4554/5554 Fall 2016</span></p><h1 class="c0 c3" id="h.e9js4aw02z33"><span class="c4"></span></h1><h1 class="c0 c7" id="h.e1p7s1rjnngg"><span class="c4">Abstract</span></h1><p class="c0"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The goal of this project was to gain understanding of PixelRNN&rsquo;s modeling capabilities. We train the PixelRNN on the CIFAR10 dataset and subsequently ask the network to optimize a new image to fit into the learned distribution. </span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 212.00px;"><img alt="" src="images/image08.png" style="width: 624.00px; height: 212.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c0 c7" id="h.rn0bahtvn33c"><span class="c4">Introduction</span></h1><p class="c0 c10"><span class="c1">Deep Learning has&mdash;in recent years&mdash;propelled the field of Computer Vision at a breakneck pace. However, a major disadvantage of these Neural Network models is that their inner workings are by and large opaque to Human inspection. Recent work by Google has provided insight into the workings of Convolutional Neural Network classifiers by using backpropagation to optimize the input signal&mdash;-rather than the weights&mdash;-to maximize the activation of a selected layer or neuron. The resulting input signals provide some visual intuition about what the network is doing. Recent work by DeepMind proposes the use of Recurrent Neural Networks for modeling the distribution of natural images. This is done by modeling the image as a discrete signal </span><img src="images/image00.png"><span class="c1">, where is a pixel value and is the dimensionality of the image. The model estimates the conditional probability for a discrete pixel value </span><img src="images/image01.png"><span class="c1">using an RNN model. We propose the application of the DeepDream methodology to the PixelRNN network architecture. Given an input image, we wish to adjust the image&rsquo;s pixel values such that it falls within the distribution learned by the PixelRNN network. </span></p><h1 class="c0 c7" id="h.3m516xawtmd2"><span class="c4">Approach</span><span>&nbsp;</span></h1><p class="c0 c10"><span class="c1">Generative image models take advantage of the fact that&mdash;although an image may be a very high-dimensional signal&mdash;the actual content of (natural) images resides on a lower-dimensional manifold. By training a neural network as a generative model, we obtain a model of the distribution of natural images. Given this model, we can perform various tasks such as sampling and imputation of missing data. Our goal is that given a trained generative model of images, we wish to present the model with an arbitrary image and have the model optimize the image such that it fits within the learned distribution.</span></p><p class="c0"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To attain this goal, we used the recently-described PixelRNN architecture. This architecture represents the image as a sequence of pixels </span><img src="images/image02.png"><span class="c1">and thus models the conditional probability for each pixel value </span><img src="images/image03.png"><span class="c1">. Given a corpus of natural images&mdash;in our case the CIFAR10 dataset&mdash;we trained an existing Tensorflow implementation of the PixelRNN architecture on this dataset. The network is trained as an autoencoder such that the optimization objective is to match the input grayscale pixel values with the output probability map. Once the network is trained, we can iteratively sample from the learned distribution by sampling from each conditional pixel value probability until we&rsquo;ve constructed a full image.</span></p><p class="c0 c10"><span class="c1">Because Tensorflow is a flexible graph-based computation environment, we can easily reconfigure the network and apply pre-trained portions of the model to new tasks. In our case, we turned the optimization problem around. Given a pre-trained model, we have the network find </span><img src="images/image04.png"><span class="c1">via gradient descent. What is unique about our approach is that we provide the network with an initial input image </span><img src="images/image05.png"><span class="c1">&nbsp;from which we apply gradient descent. That is:</span></p><p class="c0"><img src="images/image06.png"></p><p class="c0"><img src="images/image07.png"></p><p class="c0"><span class="c1">}</span></p><p class="c0"><span class="c1">In this way, we apply pixel value updates to the image until it fits within the learned distribution.</span></p><p class="c0 c12"><span class="c1"></span></p><h1 class="c0 c7" id="h.et4fj893iiv4"><span class="c4">Experiments and Results</span></h1><p class="c0"><span class="c1">Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter valu</span><span class="c1">es. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why? </span></p><p class="c0"><span class="c1"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Images were input from CIFAR-10 test set into the PixelRNN to generate the modified input image to represent the model&rsquo;s distribution. The evaluation metrics were purely looking at the residuals between the input and the modified input image and viewing the modified regions that changed. The loss for the images is zero since the input hasn&rsquo;t changed only the pixel intensities to match the model distribution.</span></p><p class="c0 c12"><span class="c1"></span></p><h1 class="c0 c7" id="h.rasdxt1zpo4s"><span class="c4">Qualitative Results</span></h1><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 212.00px;"><img alt="" src="images/image08.png" style="width: 624.00px; height: 212.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c12"><span class="c1"></span></p><p class="c0"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Qualitative results show a marked change in image regions that might be considered &ldquo;background&rdquo;. Intuitively, the CIFAR-10 dataset consists of images that represent a single subject and an irrelevant background. It appears that the model has learned this as part of the distribution of this training set, and the adjustments required to transform an input image to a conforming output image entails emphasizing the foreground subject while eliding background regions. </span></p><h1 class="c0 c7" id="h.5akumuevi1st"><span class="c4">Conclusion and Future Work</span><span>&nbsp;</span></h1><p class="c0"><span class="c1">The results have shown a priority placed on foreground images over the background images. The input dataset on CIFAR-10 has this distribution, therefore the PixelRNN has learned to model optimizing the foreground image over the background. One of the issues with our model was that we used Cross Entropy which led to optimizing the output space matching the input space with some entropy. However, this entropy not necessarily warranted and switching to optimizing the KL divergence will be sufficient. A different method of approaching this generative process would be to look further into Probabilistic Graphical Models. Appropriately perturbing the input signal to match the distribution of the generative model may yield meaningful results. </span></p><p class="c0"><span class="c4 c1">References</span><span class="c1">: </span></p><p class="c9"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/pdf/1601.06759v3.pdf&amp;sa=D&amp;ust=1481771303355000&amp;usg=AFQjCNFv7ty4tYoh9twka0K0PG_3xrlZrA">https://arxiv.org/pdf/1601.06759v3.pdf</a></span></p><p class="c9"><span class="c2"><a class="c5" href="https://www.google.com/url?q=https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html&amp;sa=D&amp;ust=1481771303356000&amp;usg=AFQjCNETPBVZC7Lq17uswAn2zGFuvh1HJw">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></span></p><p class="c9 c12"><span></span></p></body></html>